{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2ddeb14",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1770ada",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sivak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sivak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164476f4",
   "metadata": {},
   "source": [
    "# Word Tokenization Using inbuid split() Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "534b63ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Once there lived a greedy man in a small town. He was very rich, and he loved gold and all things fancy. But he loved his daughter more than anything. One day, he chanced upon a fairy. The fairy’s hair was caught in a few tree branches. He helped her out, but as his greediness took over, he realised that he had an opportunity to become richer by asking for a wish in return (by helping her out). The fairy granted him a wish.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1f7df13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Once', 'there', 'lived', 'a', 'greedy', 'man', 'in', 'a', 'small', 'town.', 'He', 'was', 'very', 'rich,', 'and', 'he', 'loved', 'gold', 'and', 'all', 'things', 'fancy.', 'But', 'he', 'loved', 'his', 'daughter', 'more', 'than', 'anything.', 'One', 'day,', 'he', 'chanced', 'upon', 'a', 'fairy.', 'The', 'fairy’s', 'hair', 'was', 'caught', 'in', 'a', 'few', 'tree', 'branches.', 'He', 'helped', 'her', 'out,', 'but', 'as', 'his', 'greediness', 'took', 'over,', 'he', 'realised', 'that', 'he', 'had', 'an', 'opportunity', 'to', 'become', 'richer', 'by', 'asking', 'for', 'a', 'wish', 'in', 'return', '(by', 'helping', 'her', 'out).', 'The', 'fairy', 'granted', 'him', 'a', 'wish.']\n"
     ]
    }
   ],
   "source": [
    "token=text.split()\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03186dc",
   "metadata": {},
   "source": [
    "# Sentence Tokenization using inbuild split() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "952f9728",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Once there lived a greedy man in a small town', ' He was very rich, and he loved gold and all things fancy', ' But he loved his daughter more than anything', ' One day, he chanced upon a fairy', ' The fairy’s hair was caught in a few tree branches', ' He helped her out, but as his greediness took over, he realised that he had an opportunity to become richer by asking for a wish in return (by helping her out)', ' The fairy granted him a wish', '']\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "token=text.split(\".\")\n",
    "print(token)\n",
    "print(len(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a614239",
   "metadata": {},
   "source": [
    "# Regular Expressions(RegEx) Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "216b6d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667be61b",
   "metadata": {},
   "source": [
    "# Word Tokenization using RegEx Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f797a73",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Once', 'there', 'lived', 'a', 'greedy', 'man', 'in', 'a', 'small', 'town', 'He', 'was', 'very', 'rich', 'and', 'he', 'loved', 'gold', 'and', 'all', 'things', 'fancy', 'But', 'he', 'loved', 'his', 'daughter', 'more', 'than', 'anything', 'One', 'day', 'he', 'chanced', 'upon', 'a', 'fairy', 'The', 'fairy', 's', 'hair', 'was', 'caught', 'in', 'a', 'few', 'tree', 'branches', 'He', 'helped', 'her', 'out', 'but', 'as', 'his', 'greediness', 'took', 'over', 'he', 'realised', 'that', 'he', 'had', 'an', 'opportunity', 'to', 'become', 'richer', 'by', 'asking', 'for', 'a', 'wish', 'in', 'return', 'by', 'helping', 'her', 'out', 'The', 'fairy', 'granted', 'him', 'a', 'wish']\n",
      "85\n"
     ]
    }
   ],
   "source": [
    "token=re.findall(\"[\\w']+\",text)\n",
    "print(token)\n",
    "print(len(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ea5546",
   "metadata": {},
   "source": [
    "# Sentence Tokenization using RegEx Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33064626",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Once there lived a greedy man in a small town', ' He was very rich, and he loved gold and all things fancy', ' But he loved his daughter more than anything', ' One day, he chanced upon a fairy', ' The fairy’s hair was caught in a few tree branches', ' He helped her out, but as his greediness took over, he realised that he had an opportunity to become richer by asking for a wish in return (by helping her out)', ' The fairy granted him a wish', '']\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "sentense=re.compile('[.?!]').split(text)\n",
    "print(sentense)\n",
    "print(len(sentense))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1468b818",
   "metadata": {},
   "source": [
    "# Tokenization Using NLTK Module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d5ca5ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sivak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0487220b",
   "metadata": {},
   "source": [
    "# Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad92e82e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Once', 'there', 'lived', 'a', 'greedy', 'man', 'in', 'a', 'small', 'town', '.', 'He', 'was', 'very', 'rich', ',', 'and', 'he', 'loved', 'gold', 'and', 'all', 'things', 'fancy', '.', 'But', 'he', 'loved', 'his', 'daughter', 'more', 'than', 'anything', '.', 'One', 'day', ',', 'he', 'chanced', 'upon', 'a', 'fairy', '.', 'The', 'fairy', '’', 's', 'hair', 'was', 'caught', 'in', 'a', 'few', 'tree', 'branches', '.', 'He', 'helped', 'her', 'out', ',', 'but', 'as', 'his', 'greediness', 'took', 'over', ',', 'he', 'realised', 'that', 'he', 'had', 'an', 'opportunity', 'to', 'become', 'richer', 'by', 'asking', 'for', 'a', 'wish', 'in', 'return', '(', 'by', 'helping', 'her', 'out', ')', '.', 'The', 'fairy', 'granted', 'him', 'a', 'wish', '.']\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "tokens=word_tokenize(text)\n",
    "print(tokens)\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d18c88",
   "metadata": {},
   "source": [
    "# Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e62703b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Once there lived a greedy man in a small town.', 'He was very rich, and he loved gold and all things fancy.', 'But he loved his daughter more than anything.', 'One day, he chanced upon a fairy.', 'The fairy’s hair was caught in a few tree branches.', 'He helped her out, but as his greediness took over, he realised that he had an opportunity to become richer by asking for a wish in return (by helping her out).', 'The fairy granted him a wish.']\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "sentence=sent_tokenize(text)\n",
    "print(sentence)\n",
    "print(len(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e34342c",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf0edaea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pythoners', 'are', 'very', 'intelligent', 'and', 'work', 'very', 'pythonly', 'and', 'now', 'they', 'are', 'pythoning', 'their', 'way', 'to', 'success', '.']\n",
      "Sentence after stemming : python are veri intellig and work veri pythonli and now they are python their way to success . \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "sentence=\"Pythoners are very intelligent and work very pythonly and now they are pythoning their way to success.\"\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "def stemSentence(sentence):\n",
    "    token_words=word_tokenize(sentence)\n",
    "    print(token_words)\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(porter.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)\n",
    "x=stemSentence(sentence)\n",
    "print(\"Sentence after stemming :\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac90d7bf",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4a1f1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sivak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b2ae8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'was', 'running', 'and', 'eating', 'at', 'same', 'time', '.', 'He', 'has', 'bad', 'habit', 'of', 'swimming', 'after', 'playing', 'long', 'hours', 'in', 'the', 'Sun', '.']\n",
      "Sentence after Lemmatization : He wa running and eating at same time . He ha bad habit of swimming after playing long hour in the Sun . \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "punctuations=\"?:!.,;\"\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "def lemmatizeSentence(sentence):\n",
    "    token_words=word_tokenize(sentence)\n",
    "    print(token_words)\n",
    "    lemma_sentence=[]\n",
    "    for word in token_words:\n",
    "        lemma_sentence.append(wordnet_lemmatizer.lemmatize(word))\n",
    "        lemma_sentence.append(\" \")\n",
    "    return \"\".join(lemma_sentence)\n",
    "x=lemmatizeSentence(sentence)\n",
    "print(\"Sentence after Lemmatization :\", x)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
